Eval num_timesteps=10000, episode_reward=56.59 +/- 25.46
Episode length: 153.72 +/- 40.77
New best mean reward!
Eval num_timesteps=20000, episode_reward=64.12 +/- 25.45
Episode length: 167.88 +/- 39.89
New best mean reward!
Eval num_timesteps=30000, episode_reward=79.65 +/- 29.75
Episode length: 204.64 +/- 55.33
New best mean reward!
Eval num_timesteps=40000, episode_reward=94.63 +/- 23.08
Episode length: 236.52 +/- 27.72
New best mean reward!
Traceback (most recent call last):
  File "loading_main.py", line 110, in <module>
    loaded_model.learn(
  File "C:\Users\cymstar_user\AppData\Local\Programs\Python\Python38\lib\site-packages\stable_baselines3\ppo\ppo.py", line 299, in learn
    return super(PPO, self).learn(
  File "C:\Users\cymstar_user\AppData\Local\Programs\Python\Python38\lib\site-packages\stable_baselines3\common\on_policy_algorithm.py", line 250, in learn
    continue_training = self.collect_rollouts(self.env, callback, self.rollout_buffer, n_rollout_steps=self.n_steps)
  File "C:\Users\cymstar_user\AppData\Local\Programs\Python\Python38\lib\site-packages\stable_baselines3\common\on_policy_algorithm.py", line 207, in collect_rollouts
    rollout_buffer.add(self._last_obs, actions, rewards, self._last_episode_starts, values, log_probs)
  File "C:\Users\cymstar_user\AppData\Local\Programs\Python\Python38\lib\site-packages\stable_baselines3\common\buffers.py", line 428, in add
    self.observations[self.pos] = np.array(obs).copy()
KeyboardInterrupt