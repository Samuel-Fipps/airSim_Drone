Eval num_timesteps=1025, episode_reward=7104.27 +/- 11.78
Episode length: 150.00 +/- 0.00
New best mean reward!
Eval num_timesteps=2050, episode_reward=-2036.72 +/- 4.57
Episode length: 51.01 +/- 0.26
Eval num_timesteps=3075, episode_reward=-2192.33 +/- 5.48
Episode length: 45.04 +/- 0.24
Eval num_timesteps=4100, episode_reward=-1847.67 +/- 10.29
Episode length: 59.88 +/- 0.35
Eval num_timesteps=5125, episode_reward=-1424.30 +/- 22.21
Episode length: 73.64 +/- 0.54
Eval num_timesteps=6150, episode_reward=-1508.22 +/- 36.17
Episode length: 71.95 +/- 0.74
Eval num_timesteps=7175, episode_reward=1101.26 +/- 44.54
Episode length: 117.14 +/- 0.57
Eval num_timesteps=8200, episode_reward=-1633.84 +/- 14.93
Episode length: 70.87 +/- 0.44
Eval num_timesteps=9225, episode_reward=-1485.58 +/- 13.86
Episode length: 83.93 +/- 0.29
Eval num_timesteps=10250, episode_reward=8683.71 +/- 12.53
Episode length: 150.00 +/- 0.00
New best mean reward!
Eval num_timesteps=11275, episode_reward=-2101.70 +/- 19.84
Episode length: 48.52 +/- 0.50
Eval num_timesteps=12300, episode_reward=912.83 +/- 42.98
Episode length: 115.05 +/- 0.52
Eval num_timesteps=13325, episode_reward=-2024.60 +/- 19.62
Episode length: 53.94 +/- 0.28
Eval num_timesteps=14350, episode_reward=-890.30 +/- 32.22
Episode length: 89.64 +/- 0.56
Eval num_timesteps=15375, episode_reward=-2167.22 +/- 21.57
Episode length: 45.99 +/- 0.17
Eval num_timesteps=16400, episode_reward=-2079.13 +/- 7.51
Episode length: 52.19 +/- 0.39
Eval num_timesteps=17425, episode_reward=9350.91 +/- 21.05
Episode length: 150.00 +/- 0.00
New best mean reward!
Eval num_timesteps=18450, episode_reward=-1880.52 +/- 11.72
Episode length: 61.31 +/- 0.46
Eval num_timesteps=19475, episode_reward=8528.71 +/- 20.41
Episode length: 150.00 +/- 0.00
Eval num_timesteps=20500, episode_reward=8486.75 +/- 12.81
Episode length: 150.00 +/- 0.00
Eval num_timesteps=21525, episode_reward=8966.54 +/- 21.14
Episode length: 150.00 +/- 0.00
Eval num_timesteps=22550, episode_reward=189.53 +/- 41.24
Episode length: 112.89 +/- 0.61
Eval num_timesteps=23575, episode_reward=-1947.56 +/- 14.25
Episode length: 62.48 +/- 0.56
Eval num_timesteps=24600, episode_reward=-550.93 +/- 25.89
Episode length: 103.26 +/- 0.44
Eval num_timesteps=25625, episode_reward=2101.47 +/- 60.29
Episode length: 137.01 +/- 0.64
Eval num_timesteps=26650, episode_reward=526.05 +/- 38.70
Episode length: 114.88 +/- 0.52
Eval num_timesteps=27675, episode_reward=8550.34 +/- 15.91
Episode length: 150.00 +/- 0.00
Eval num_timesteps=28700, episode_reward=8210.69 +/- 20.61
Episode length: 150.00 +/- 0.00
Eval num_timesteps=29725, episode_reward=-2090.06 +/- 9.66
Episode length: 53.62 +/- 0.49
Eval num_timesteps=30750, episode_reward=9148.52 +/- 22.13
Episode length: 150.00 +/- 0.00
Eval num_timesteps=31775, episode_reward=-2039.34 +/- 13.90
Episode length: 63.32 +/- 0.61
Eval num_timesteps=32800, episode_reward=-1530.74 +/- 20.77
Episode length: 81.43 +/- 0.55
Eval num_timesteps=33825, episode_reward=636.58 +/- 2369.08
Episode length: 112.63 +/- 13.15
Eval num_timesteps=34850, episode_reward=-2150.56 +/- 21.41
Episode length: 50.38 +/- 0.51
Eval num_timesteps=35875, episode_reward=8657.85 +/- 18.86
Episode length: 150.00 +/- 0.00
Eval num_timesteps=36900, episode_reward=-1512.87 +/- 33.89
Episode length: 78.45 +/- 0.83
Eval num_timesteps=37925, episode_reward=258.56 +/- 32.61
Episode length: 114.24 +/- 0.45
Eval num_timesteps=38950, episode_reward=8936.89 +/- 20.67
Episode length: 150.00 +/- 0.00
Eval num_timesteps=39975, episode_reward=9241.39 +/- 21.92
Episode length: 150.00 +/- 0.00
Eval num_timesteps=41000, episode_reward=-1940.06 +/- 9.03
Episode length: 61.89 +/- 0.37
Eval num_timesteps=42025, episode_reward=-1783.22 +/- 19.71
Episode length: 68.44 +/- 0.55
Eval num_timesteps=43050, episode_reward=9568.32 +/- 24.22
Episode length: 150.00 +/- 0.00
New best mean reward!
Eval num_timesteps=44075, episode_reward=-1884.57 +/- 11.35
Episode length: 60.79 +/- 0.45
Eval num_timesteps=45100, episode_reward=273.28 +/- 36.26
Episode length: 115.54 +/- 0.52
No picuture
Eval num_timesteps=46125, episode_reward=8466.08 +/- 8.88
Episode length: 150.00 +/- 0.00
Eval num_timesteps=47150, episode_reward=3479.70 +/- 62.59
Episode length: 142.13 +/- 0.58
Eval num_timesteps=48175, episode_reward=8845.93 +/- 16.04
Episode length: 150.00 +/- 0.00
Eval num_timesteps=49200, episode_reward=8916.09 +/- 20.99
Episode length: 150.00 +/- 0.00
Eval num_timesteps=50225, episode_reward=9047.53 +/- 23.29
Episode length: 150.00 +/- 0.00
Eval num_timesteps=51250, episode_reward=8518.67 +/- 13.72
Episode length: 150.00 +/- 0.00
Traceback (most recent call last):
  File "f:\Everything\New_drone\main.py", line 214, in <module>
    model.learn(
  File "C:\Users\14055\AppData\Local\Programs\Python\Python39\lib\site-packages\stable_baselines3\ppo\ppo.py", line 299, in learn
    return super(PPO, self).learn(
  File "C:\Users\14055\AppData\Local\Programs\Python\Python39\lib\site-packages\stable_baselines3\common\on_policy_algorithm.py", line 251, in learn
    continue_training = self.collect_rollouts(self.env, callback, self.rollout_buffer, n_rollout_steps=self.n_steps)
  File "C:\Users\14055\AppData\Local\Programs\Python\Python39\lib\site-packages\stable_baselines3\common\on_policy_algorithm.py", line 185, in collect_rollouts
    if callback.on_step() is False:
  File "C:\Users\14055\AppData\Local\Programs\Python\Python39\lib\site-packages\stable_baselines3\common\callbacks.py", line 88, in on_step
    return self._on_step()
  File "C:\Users\14055\AppData\Local\Programs\Python\Python39\lib\site-packages\stable_baselines3\common\callbacks.py", line 192, in _on_step
    continue_training = callback.on_step() and continue_training
  File "C:\Users\14055\AppData\Local\Programs\Python\Python39\lib\site-packages\stable_baselines3\common\callbacks.py", line 88, in on_step
    return self._on_step()
  File "f:\Everything\New_drone\main.py", line 34, in _on_step
    super()._on_step()
  File "C:\Users\14055\AppData\Local\Programs\Python\Python39\lib\site-packages\stable_baselines3\common\callbacks.py", line 376, in _on_step
    episode_rewards, episode_lengths = evaluate_policy(
  File "C:\Users\14055\AppData\Local\Programs\Python\Python39\lib\site-packages\stable_baselines3\common\evaluation.py", line 87, in evaluate_policy
    observations, rewards, dones, infos = env.step(actions)
  File "C:\Users\14055\AppData\Local\Programs\Python\Python39\lib\site-packages\stable_baselines3\common\vec_env\base_vec_env.py", line 162, in step
    return self.step_wait()
  File "C:\Users\14055\AppData\Local\Programs\Python\Python39\lib\site-packages\stable_baselines3\common\vec_env\vec_transpose.py", line 95, in step_wait
    observations, rewards, dones, infos = self.venv.step_wait()
  File "C:\Users\14055\AppData\Local\Programs\Python\Python39\lib\site-packages\stable_baselines3\common\vec_env\dummy_vec_env.py", line 43, in step_wait
    obs, self.buf_rews[env_idx], self.buf_dones[env_idx], self.buf_infos[env_idx] = self.envs[env_idx].step(
  File "C:\Users\14055\AppData\Local\Programs\Python\Python39\lib\site-packages\stable_baselines3\common\monitor.py", line 90, in step
    observation, reward, done, info = self.env.step(action)
  File "f:\Everything\New_drone\scripts\airsim_env.py", line 77, in step
    self.do_action(action)
  File "f:\Everything\New_drone\scripts\airsim_env.py", line 128, in do_action
    self.drone.moveByVelocityAsync(vx, vy, vz, duration=0.1).join()
  File "C:\Users\14055\AppData\Local\Programs\Python\Python39\lib\site-packages\msgpackrpc\future.py", line 22, in join
    self._loop.start()
  File "C:\Users\14055\AppData\Local\Programs\Python\Python39\lib\site-packages\msgpackrpc\loop.py", line 22, in start
    self._ioloop.start()
  File "C:\Users\14055\AppData\Local\Programs\Python\Python39\lib\site-packages\tornado\ioloop.py", line 863, in start
    event_pairs = self._impl.poll(poll_timeout)
  File "C:\Users\14055\AppData\Local\Programs\Python\Python39\lib\site-packages\tornado\platform\select.py", line 62, in poll
    readable, writeable, errors = select.select(
KeyboardInterrupt