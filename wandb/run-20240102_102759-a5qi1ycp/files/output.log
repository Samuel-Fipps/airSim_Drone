Eval num_timesteps=10000, episode_reward=-157.91 +/- 15.39
Episode length: 7.36 +/- 0.48
New best mean reward!
Eval num_timesteps=20000, episode_reward=-161.40 +/- 15.78
Episode length: 7.48 +/- 0.50
Eval num_timesteps=30000, episode_reward=-152.10 +/- 12.70
Episode length: 7.20 +/- 0.40
New best mean reward!
Eval num_timesteps=40000, episode_reward=-155.89 +/- 1.70
Episode length: 7.00 +/- 0.00
Eval num_timesteps=50000, episode_reward=-155.11 +/- 0.99
Episode length: 7.00 +/- 0.00
Eval num_timesteps=60000, episode_reward=-152.55 +/- 12.52
Episode length: 7.20 +/- 0.40
Eval num_timesteps=70000, episode_reward=-174.70 +/- 8.52
Episode length: 7.92 +/- 0.27
Eval num_timesteps=80000, episode_reward=-174.45 +/- 8.74
Episode length: 7.92 +/- 0.27
Eval num_timesteps=90000, episode_reward=-169.84 +/- 13.18
Episode length: 7.76 +/- 0.43
Eval num_timesteps=100000, episode_reward=-164.91 +/- 15.33
Episode length: 7.60 +/- 0.49
Eval num_timesteps=110000, episode_reward=-172.57 +/- 11.72
Episode length: 7.84 +/- 0.37
Eval num_timesteps=120000, episode_reward=-175.28 +/- 8.67
Episode length: 7.92 +/- 0.27
Eval num_timesteps=130000, episode_reward=-167.74 +/- 14.70
Episode length: 7.68 +/- 0.47
Eval num_timesteps=140000, episode_reward=-172.29 +/- 11.74
Episode length: 7.84 +/- 0.37
Eval num_timesteps=150000, episode_reward=-162.23 +/- 15.64
Episode length: 7.52 +/- 0.50
Traceback (most recent call last):
  File "F:\Everything\New_drone\loading_main.py", line 110, in <module>
    loaded_model.learn(
  File "C:\Users\14055\AppData\Local\Programs\Python\Python39\lib\site-packages\stable_baselines3\ppo\ppo.py", line 299, in learn
    return super(PPO, self).learn(
  File "C:\Users\14055\AppData\Local\Programs\Python\Python39\lib\site-packages\stable_baselines3\common\on_policy_algorithm.py", line 251, in learn
    continue_training = self.collect_rollouts(self.env, callback, self.rollout_buffer, n_rollout_steps=self.n_steps)
  File "C:\Users\14055\AppData\Local\Programs\Python\Python39\lib\site-packages\stable_baselines3\common\on_policy_algorithm.py", line 179, in collect_rollouts
    new_obs, rewards, dones, infos = env.step(clipped_actions)
  File "C:\Users\14055\AppData\Local\Programs\Python\Python39\lib\site-packages\stable_baselines3\common\vec_env\base_vec_env.py", line 162, in step
    return self.step_wait()
  File "C:\Users\14055\AppData\Local\Programs\Python\Python39\lib\site-packages\stable_baselines3\common\vec_env\vec_transpose.py", line 95, in step_wait
    observations, rewards, dones, infos = self.venv.step_wait()
  File "C:\Users\14055\AppData\Local\Programs\Python\Python39\lib\site-packages\stable_baselines3\common\vec_env\dummy_vec_env.py", line 49, in step_wait
    obs = self.envs[env_idx].reset()
  File "C:\Users\14055\AppData\Local\Programs\Python\Python39\lib\site-packages\stable_baselines3\common\monitor.py", line 79, in reset
    return self.env.reset(**kwargs)
  File "F:\Everything\New_drone\scripts\airsim_env.py", line 81, in reset
    self.setup_flight()
  File "F:\Everything\New_drone\scripts\airsim_env.py", line 100, in setup_flight
    self.drone.moveByRollPitchYawrateZAsync(0, 0, 6.3, self.drone.getMultirotorState().kinematics_estimated.position.z_val, 1).join()
  File "C:\Users\14055\AppData\Local\Programs\Python\Python39\lib\site-packages\msgpackrpc\future.py", line 22, in join
    self._loop.start()
  File "C:\Users\14055\AppData\Local\Programs\Python\Python39\lib\site-packages\msgpackrpc\loop.py", line 22, in start
    self._ioloop.start()
  File "C:\Users\14055\AppData\Local\Programs\Python\Python39\lib\site-packages\tornado\ioloop.py", line 863, in start
    event_pairs = self._impl.poll(poll_timeout)
  File "C:\Users\14055\AppData\Local\Programs\Python\Python39\lib\site-packages\tornado\platform\select.py", line 62, in poll
    readable, writeable, errors = select.select(
KeyboardInterrupt